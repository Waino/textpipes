[exp]
n_lines = 1000
seed = 1

[subconf]
template1 = template.ini
template2 = template.ini

[subconf.template]
template1 = #FOO#=1
template2 = #FOO#=2

[paths.dirs]
tmp = tmp
inputs = inputs
stable = out_gen_stable
unstable = out_gen_unstable

[paths.inputs]
count_tokens = ${paths.dirs:inputs}/count
count_tokens2 = ${paths.dirs:inputs}/count2
count_chars = ${paths.dirs:inputs}/count

foreign = ${paths.dirs:inputs}/foreign
foreign_foreign = ${paths.dirs:inputs}/foreign_chars
omorfi_normalize = ${paths.dirs:inputs}/omorfi_normalize

train_truecaser = ${paths.dirs:inputs}/train_truecaser
truecase = ${paths.dirs:inputs}/truecase
lowercase = ${paths.dirs:inputs}/lowercase

reencode = ${paths.dirs:inputs}/reencode

remove_language_tags = ${paths.dirs:inputs}/remove_language_tags

drop_tokens = ${paths.dirs:inputs}/noise
peturb_order = ${paths.dirs:inputs}/noise
seg_peturb_order = ${paths.dirs:inputs}/seg_peturb_order

truncate_words = ${paths.dirs:inputs}/train_truecaser

onmt_tokenize = ${paths.dirs:inputs}/tokenize
simple_tokenize = ${paths.dirs:inputs}/tokenize
old_tokenize = ${paths.dirs:inputs}/tokenize
force_tok_long_num = ${paths.dirs:inputs}/force_tok_long_num

apply_segmentation = ${paths.dirs:inputs}/apply_segmentation
apply_segmentation_seg = ${paths.dirs:inputs}/apply_segmentation_seg

gzip = ${paths.dirs:inputs}/gzip.gz

transparent_tmp = ${paths.dirs:inputs}/noise

[paths.outputs]
count_tokens = ${paths.dirs:stable}/count_tokens
count_tokens2 = ${paths.dirs:stable}/count_tokens2
count_tokens2_words = ${paths.dirs:stable}/count_tokens2_words
count_chars = ${paths.dirs:stable}/count_chars
remove_counts = ${paths.dirs:stable}/remove_counts
scale_counts = ${paths.dirs:stable}/scale_counts
combine_counts = ${paths.dirs:stable}/combine_counts
combine_counts_balance = ${paths.dirs:stable}/combine_counts_balance
combine_wordlists = ${paths.dirs:stable}/combine_wordlists

foreign = ${paths.dirs:stable}/foreign
omorfi_normalize = ${paths.dirs:stable}/omorfi_normalize
omorfi_normalize_nonconc = ${paths.dirs:stable}/omorfi_normalize_nonconc

train_truecaser = ${paths.dirs:stable}/train_truecaser
truecase = ${paths.dirs:stable}/truecase
detruecase = ${paths.dirs:stable}/detruecase
lowercase = ${paths.dirs:stable}/lowercase

reencode = ${paths.dirs:stable}/reencode

train_morfessor = ${paths.dirs:unstable}/train_morfessor
morfessor.lexicon = ${paths.dirs:unstable}/morfessor.lexicon
morfessor.params = ${paths.dirs:unstable}/morfessor.params

remove_language_tags = ${paths.dirs:stable}/remove_language_tags

drop_tokens = ${paths.dirs:unstable}/drop_tokens
peturb_order = ${paths.dirs:unstable}/peturb_order
seg_peturb_order = ${paths.dirs:unstable}/seg_peturb_order

truncate_words = ${paths.dirs:stable}/truncate_words

onmt_tokenize = ${paths.dirs:stable}/onmt_tokenize
simple_tokenize = ${paths.dirs:stable}/simple_tokenize
old_tokenize = ${paths.dirs:stable}/old_tokenize
onmt_detokenize = ${paths.dirs:stable}/onmt_detokenize
simple_detokenize = ${paths.dirs:stable}/simple_detokenize
old_detokenize = ${paths.dirs:stable}/old_detokenize
force_tok_long_num = ${paths.dirs:stable}/force_tok_long_num

apply_segmentation = ${paths.dirs:stable}/apply_segmentation
apply_segmentation_missing = ${paths.dirs:stable}/apply_segmentation_missing

char_segmentation = ${paths.dirs:stable}/char_segmentation

gzip = ${paths.dirs:stable}/gzip.gz

transparent_tmp = ${paths.dirs:stable}/transparent_tmp
